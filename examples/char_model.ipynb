{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "import os\n",
    "import simplejson as json\n",
    "apollo_root = os.environ['APOLLO_ROOT']\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import apollo\n",
    "import logging\n",
    "from apollo import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "with open('%s/data/language_model/vocab.pkl' % os.environ['APOLLO_ROOT'], 'r') as f:\n",
    "    vocab = pickle.load(f)\n",
    "ivocab = {v: k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_hyper():\n",
    "    hyper = {}\n",
    "    hyper['vocab_size'] = 256\n",
    "    hyper['batch_size'] = 32\n",
    "    hyper['init_range'] = 0.1\n",
    "    hyper['zero_symbol'] = hyper['vocab_size'] - 1\n",
    "    hyper['unknown_symbol'] = hyper['vocab_size'] - 2\n",
    "    hyper['test_interval'] = 100\n",
    "    hyper['test_iter'] = 20\n",
    "    hyper['base_lr'] = 20\n",
    "    hyper['weight_decay'] = 0\n",
    "    hyper['momentum'] = 0.0\n",
    "    hyper['clip_gradients'] = 0.24\n",
    "    hyper['display_interval'] = 100\n",
    "    hyper['max_iter'] = 10000\n",
    "    hyper['snapshot_prefix'] = '/tmp/char'\n",
    "    hyper['snapshot_interval'] = 1000\n",
    "    hyper['random_seed'] = 22\n",
    "    hyper['gamma'] = 0.8\n",
    "    hyper['graph_interval'] = 1000\n",
    "    hyper['stepsize'] = 2500\n",
    "    hyper['mem_cells'] = 1000\n",
    "\n",
    "    hyper['graph_interval'] = 1000\n",
    "    hyper['graph_prefix'] = ''\n",
    "    hyper['i_temperature'] = 1.5\n",
    "    return hyper\n",
    "\n",
    "hyper = get_hyper()\n",
    "\n",
    "apollo.Caffe.set_random_seed(hyper['random_seed'])\n",
    "apollo.Caffe.set_mode_gpu()\n",
    "apollo.Caffe.set_device(1)\n",
    "apollo.Caffe.set_logging_verbosity(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    data_source = '%s/data/char_model/reddit_ml.txt' % apollo_root\n",
    "    if not os.path.exists(data_source):\n",
    "        raise IOError('You must download the data with ./data/character_model/get_reddit_lm.sh')\n",
    "    epoch = 0\n",
    "    while True:\n",
    "        with open(data_source, 'r') as f:\n",
    "            for x in f.readlines():\n",
    "                data = json.loads(x)\n",
    "                if len(data['body']) == 0:\n",
    "                    continue\n",
    "                yield data\n",
    "        logging.info('epoch %s finished' % epoch)\n",
    "        epoch += 1\n",
    "\n",
    "def get_data_batch(data_iter):\n",
    "    while True:\n",
    "        batch = []\n",
    "        for i in range(hyper['batch_size']):\n",
    "            batch.append(next(data_iter))\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pad_batch(sentence_batch):\n",
    "    max_len = max(len(x) for x in sentence_batch)\n",
    "    result = []\n",
    "    for sentence in sentence_batch:\n",
    "        chars = [min(ord(c), 255) for c in sentence] \n",
    "        result.append(chars + [hyper['zero_symbol']] * (max_len - len(sentence)))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def forward(net, sentence_batches):\n",
    "    batch = next(sentence_batches)\n",
    "    sentence_batch = np.array(pad_batch([x['body'] for x in batch]))\n",
    "    length = min(sentence_batch.shape[1], 100)\n",
    "    assert length > 0\n",
    "\n",
    "    filler = layers.Filler(type='uniform', max=hyper['init_range'],\n",
    "        min=(-hyper['init_range']))\n",
    "    net.forward_layer(layers.NumpyData(name='lstm_seed',\n",
    "        data=np.zeros((hyper['batch_size'], hyper['mem_cells'], 1, 1))))\n",
    "    net.forward_layer(layers.NumpyData(name='label',\n",
    "        data=np.zeros((hyper['batch_size'] * length, 1, 1, 1))))\n",
    "    hidden_concat_bottoms = []\n",
    "    for step in range(length):\n",
    "        net.forward_layer(layers.DummyData(name=('word%d' % step),\n",
    "            shape=[hyper['batch_size'], 1, 1, 1]))\n",
    "        if step == 0:\n",
    "            prev_hidden = 'lstm_seed'\n",
    "            prev_mem = 'lstm_seed'\n",
    "            word = np.zeros(sentence_batch[:, 0].shape)\n",
    "        else:\n",
    "            prev_hidden = 'lstm%d_hidden' % (step - 1)\n",
    "            prev_mem = 'lstm%d_mem' % (step - 1)\n",
    "            word = sentence_batch[:, step - 1]\n",
    "        net.tops['word%d' % step].data[:,0,0,0] = word\n",
    "        net.forward_layer(layers.Wordvec(name=('wordvec%d' % step),\n",
    "            bottoms=['word%d' % step],\n",
    "            dimension=hyper['mem_cells'], vocab_size=hyper['vocab_size'],\n",
    "            param_names=['wordvec_param'], weight_filler=filler))\n",
    "        net.forward_layer(layers.Concat(name='lstm_concat%d' % step,\n",
    "            bottoms=[prev_hidden, 'wordvec%d' % step]))\n",
    "        net.forward_layer(layers.Lstm(name='lstm%d' % step,\n",
    "            bottoms=['lstm_concat%d' % step, prev_mem],\n",
    "            param_names=['lstm_input_value', 'lstm_input_gate',\n",
    "                'lstm_forget_gate', 'lstm_output_gate'],\n",
    "            tops=['lstm%d_hidden' % step, 'lstm%d_mem' % step],\n",
    "            num_cells=hyper['mem_cells'], weight_filler=filler))\n",
    "        net.forward_layer(layers.Dropout(name='dropout%d' % step,\n",
    "            bottoms=['lstm%d_hidden' % step], dropout_ratio=0.16))\n",
    "        hidden_concat_bottoms.append('dropout%d' % step)\n",
    "\n",
    "    net.forward_layer(layers.Concat(name='hidden_concat',\n",
    "        concat_dim=0, bottoms=hidden_concat_bottoms))\n",
    "    net.tops['label'].data[:,0,0,0] = sentence_batch[:, :length].T.flatten()\n",
    "    net.forward_layer(layers.InnerProduct(name='ip', bottoms=['hidden_concat'],\n",
    "        num_output=hyper['vocab_size'], weight_filler=filler))\n",
    "    loss = net.forward_layer(layers.SoftmaxWithLoss(name='softmax_loss',\n",
    "        ignore_label=hyper['zero_symbol'], bottoms=['ip', 'label']))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def eval_performance(net):\n",
    "    eval_net = apollo.Net()\n",
    "    eval_forward(eval_net)\n",
    "    eval_net.copy_params_from(net)\n",
    "    output_words = eval_forward(eval_net)\n",
    "    print ''.join([chr(x) for x in output_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def softmax_choice(data):\n",
    "    return np.random.choice(range(len(data.flatten())), p=data.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_forward(net):\n",
    "    output_words = []\n",
    "    filler = layers.Filler(type='uniform', max=hyper['init_range'],\n",
    "        min=(-hyper['init_range']))\n",
    "    net.forward_layer(layers.NumpyData(name='lstm_hidden_prev',\n",
    "        data=np.zeros((1, hyper['mem_cells'], 1, 1))))\n",
    "    net.forward_layer(layers.NumpyData(name='lstm_mem_prev',\n",
    "        data=np.zeros((1, hyper['mem_cells'], 1, 1))))\n",
    "    length = 150\n",
    "    for step in range(length):\n",
    "        net.forward_layer(layers.NumpyData(name=('word'),\n",
    "            data=np.zeros((1, 1, 1, 1))))\n",
    "        prev_hidden = 'lstm_hidden_prev'\n",
    "        prev_mem = 'lstm_mem_prev'\n",
    "        word = np.zeros((1, 1, 1, 1))\n",
    "        if step == 0:\n",
    "            output = ord('.')\n",
    "        else:\n",
    "            output = softmax_choice(net.tops['softmax'].data)\n",
    "        output_words.append(output)\n",
    "        net.tops['word'].data[0,0,0,0] = output\n",
    "        net.forward_layer(layers.Wordvec(name=('wordvec'),\n",
    "            bottoms=['word'],\n",
    "            dimension=hyper['mem_cells'], vocab_size=hyper['vocab_size'],\n",
    "            param_names=['wordvec_param'], weight_filler=filler))\n",
    "        net.forward_layer(layers.Concat(name='lstm_concat',\n",
    "            bottoms=[prev_hidden, 'wordvec']))\n",
    "        net.forward_layer(layers.Lstm(name='lstm',\n",
    "            bottoms=['lstm_concat', prev_mem],\n",
    "            param_names=['lstm_input_value', 'lstm_input_gate',\n",
    "                'lstm_forget_gate', 'lstm_output_gate'],\n",
    "            tops=['lstm_hidden_next', 'lstm_mem_next'],\n",
    "            num_cells=hyper['mem_cells'], weight_filler=filler))\n",
    "        net.forward_layer(layers.Dropout(name='dropout',\n",
    "            bottoms=['lstm_hidden_next'], dropout_ratio=0.16))\n",
    "\n",
    "        net.forward_layer(layers.InnerProduct(name='ip', bottoms=['dropout'],\n",
    "            num_output=hyper['vocab_size'], weight_filler=filler))\n",
    "        net.tops['ip'].data[:] *= hyper['i_temperature']\n",
    "        net.forward_layer(layers.Softmax(name='softmax',\n",
    "            ignore_label=hyper['zero_symbol'], bottoms=['ip']))\n",
    "        net.tops['lstm_hidden_prev'].data_tensor.copy_from(net.tops['lstm_hidden_next'].data_tensor)\n",
    "        net.tops['lstm_mem_prev'].data_tensor.copy_from(net.tops['lstm_mem_next'].data_tensor)\n",
    "        net.reset_forward()\n",
    "    return output_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = apollo.Net()\n",
    "\n",
    "apollo.log.log_to_stdout() # for ipython notebook\n",
    "sentences = get_data()\n",
    "sentence_batches = get_data_batch(sentences)\n",
    "\n",
    "forward(net, sentence_batches)\n",
    "net.reset_forward()\n",
    "train_loss_hist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-07-14 16:31:25,870 - INFO - Iteration 0: 5.55461072922\n",
      ".\u0014\u0014M���Y�$\u0017caE`i���S\n",
      "I�ŝW�&,4\u000b",
      "ǝ��y9 ��'$���)\u001a8~\u0000\t�\u001e",
      "L\u0018�k�,��D�\t\u001a�挞� �\u001b�[u�\u0018eȎ�u� � p���ru%���\u0007r\n",
      "D�x�]wz8�{e �s\u0006?���O�u���ƛ�'��pl��  r  e�ao '\n",
      "2015-07-14 16:32:00,581 - INFO - Iteration 100: 4.48616950274\n",
      ".. Itthessimeallyousthis\u0006usterelthiseredithessielloouthereni�eturexicemeteesealestichisimelestilly therenecreacliouiestirearetiseallestores, freaechet\n",
      "2015-07-14 16:32:37,788 - INFO - Iteration 200: 2.5186178422\n",
      ". Thinks of inperimech of you beanding \"normally and like ingerestime \" problemaring mabuling some rearning (andersticking don't formands and of have \n",
      "2015-07-14 16:33:14,259 - INFO - Iteration 300: 2.02955379009\n",
      ". I coversultions, toologoodifientoonont/station/onenotitionasioning. Confinations. oftingterfortuations. Explaining, there tools.it/ontoro/in/gotiona\n",
      "2015-07-14 16:33:50,748 - INFO - Iteration 400: 1.80005491734\n",
      ".  I want a data is ration is a many not do you main function as surp of s a sparse that is with a rachine line a looks a facirial find a normal now m\n",
      "2015-07-14 16:34:26,617 - INFO - Iteration 500: 1.66021199465\n",
      ". They a say to do required without they have to stap the paper with not is they many to do scientist of the top me so the best to use R ML of them on\n",
      "2015-07-14 16:35:01,808 - INFO - Iteration 600: 1.58152794003\n",
      ". At more before is nice the convolutional setabile use the field is on extracted to the propagation is a detaile sees a sounds learning all the don't\n",
      "2015-07-14 16:35:38,866 - INFO - Iteration 700: 1.53211682081\n",
      ". The arguably contributions, you might probably that now search first it eesuarides? That the statistics. That's a simple and it is a Cheth This is a\n",
      "2015-07-14 16:36:14,264 - INFO - Iteration 800: 1.48265722752\n",
      ". To be doff with the paper is to talking about this post of a similar competition of try tool progressive of the ideasets to done to able to do, the \n",
      "2015-07-14 16:36:50,282 - INFO - Iteration 900: 1.45898468614\n",
      ". It was a single as a few then I formed anyway at least if you need to that it like this looks like there are a field, but I'm good in the time and e\n",
      "2015-07-14 16:37:25,294 - INFO - Iteration 1000: 1.44349015594\n",
      ". I can want to do it in the following around main call in the NLP decision metric category case but I was pretty interested in many of the neural net\n",
      "2015-07-14 16:37:25,687 - INFO - Saving net to: /tmp/char_1000.h5\n",
      "2015-07-14 16:37:25,806 - INFO - Saving figure to: train_loss.jpg\n",
      "2015-07-14 16:38:00,000 - INFO - Iteration 1100: 1.40290100574\n",
      ". A couple of statistion. I don't mean that and reason about the subject is supervised feedbox is the biole see of the research of the questions is th\n",
      "2015-07-14 16:38:35,434 - INFO - Iteration 1200: 1.39800467849\n",
      "... every should be look at authors and user's a mange perfect improved, but enceparameters in model for very machines, and don't personally delibe th\n",
      "2015-07-14 16:39:11,126 - INFO - Iteration 1300: 1.39177626729\n",
      ". I have a convolutions and confirms to rool and learning math example of the paper and stuff a weights for on the form as I was just go faster of a n\n",
      "2015-07-14 16:39:45,962 - INFO - Iteration 1400: 1.36899568558\n",
      ". The rest prior and I am nuced to the examples of a predictions. The videos for the sense, training on the book and a word and the video details abou\n",
      "2015-07-14 16:40:20,909 - INFO - Iteration 1500: 1.36260091662\n",
      ". I am great. The rather areas comperiomations or the same state-problem I have a lot of the packages metaips a model that working out, in the results\n",
      "2015-07-14 16:40:56,416 - INFO - Iteration 1600: 1.35607215762\n",
      ". I don't have any difference in another comments. If you want to sort of who can start from a hune of academia on classifier or most assuming that so\n",
      "2015-07-14 16:41:31,342 - INFO - Iteration 1700: 1.33420743465\n",
      ". If your preprocess of the need to clear the students, and we believe that I wish some different than the most memory is this benaries there. It real\n",
      "2015-07-14 16:41:43,155 - INFO - epoch 0 finished\n",
      "2015-07-14 16:42:07,562 - INFO - Iteration 1800: 1.33507900119\n",
      ".  The problem because that the biggest parts of number of response to the filters are describes another gradient between Calculus between taught tage\n",
      "2015-07-14 16:42:42,675 - INFO - Iteration 1900: 1.32881636262\n",
      "...for the deeplearning logic and features deep learning about delay of the problem of the all the poorestills/values at least weight and about hither\n",
      "2015-07-14 16:43:19,426 - INFO - Iteration 2000: 1.31548191905\n",
      ". It looks really a gning algorithms are set in the transfer algorithm http://www.coursera.org/spots/101/10/01/12/29/ailement into your decision tries\n",
      "2015-07-14 16:43:19,965 - INFO - Saving net to: /tmp/char_2000.h5\n",
      "2015-07-14 16:43:19,995 - INFO - Saving figure to: train_loss.jpg\n",
      "2015-07-14 16:43:57,126 - INFO - Iteration 2100: 1.30683990717\n",
      ".A lot.  It's a really largely applications of hierarchy autoencoder that you could be related to the 3 without lotists to ML and the article is not s\n",
      "2015-07-14 16:44:33,931 - INFO - Iteration 2200: 1.30016347289\n",
      ". I'm working on it worth. At the top to my question is so much for an expert an interesting is the student is the best thoughts than experience and t\n",
      "2015-07-14 16:45:11,726 - INFO - Iteration 2300: 1.29247198462\n",
      ". This isn't the recursive to unsupervised metrics or the demo on gend (some problem? It looks like the videos?\n",
      "\n",
      "http://accuracyetic-seriously.com/det\n",
      "2015-07-14 16:45:46,574 - INFO - Iteration 2400: 1.27350085258\n",
      ". http://far.wikipedia.org/wiki/Allbots.pdf) as a concepts and train only simply the classifiers of this which link like the topic mathematical accura\n",
      "2015-07-14 16:46:20,996 - INFO - Iteration 2500: 1.27536597371\n",
      ".  The right good look into the linearly more for it, and how are you currently not considering the hidden layer is mailing for equivalent and the dif\n",
      "2015-07-14 16:46:56,182 - INFO - Iteration 2600: 1.24221874356\n",
      ".\n",
      "\n",
      "Yes.  I have no idea and also answering it is not an explanation of the articles on the signal problem and don't mention so there are any stuff cal\n",
      "2015-07-14 16:47:33,406 - INFO - Iteration 2700: 1.24667096019\n",
      ". I'm a math point on the modern different distribution in a solution to solve the about the convertion of my addition for the algorithm to even check\n",
      "2015-07-14 16:48:11,949 - INFO - Iteration 2800: 1.23139879942\n",
      ". I agree that it would be better off samples instead of the work on a segmentations are like that the reinforcement learning too, yes. What I had a c\n",
      "2015-07-14 16:48:49,270 - INFO - Iteration 2900: 1.23162234068\n",
      ". However, helpful for a set of a stream or simply the article with a lot of inspiring the papers is more about the basic function is pretty good.  Fo\n",
      "2015-07-14 16:49:27,797 - INFO - Iteration 3000: 1.23618363142\n",
      ". But if you are wondering what has to compare the (internative for representation is a band the problem of thought of a better of the company thing i\n",
      "2015-07-14 16:49:28,303 - INFO - Saving net to: /tmp/char_3000.h5\n",
      "2015-07-14 16:49:28,330 - INFO - Saving figure to: train_loss.jpg\n",
      "2015-07-14 16:50:06,683 - INFO - Iteration 3100: 1.22520297408\n",
      "... Paradigm like a look at all examples have simpler to recall it the same thing I am not a big and or in our stand use a late with comparative nets \n",
      "2015-07-14 16:50:43,258 - INFO - Iteration 3200: 1.22753942132\n",
      ". All I was a good idea if you're interested in this suggestions, you're not interested in cancy and one of the most decision trees in a algorithm in \n",
      "2015-07-14 16:51:19,943 - INFO - Iteration 3300: 1.22033241153\n",
      ". The idea is the ground threads do that has the fact.\n",
      "\n",
      "This is more interesting. I added the original standard will other this seems to be hongsted t\n",
      "2015-07-14 16:51:55,927 - INFO - Iteration 3400: 1.21509530067\n",
      ". You can encode the source for the final algorithm that have something that doesn't seem to wait in the links. I want to find all of the slides I don\n",
      "2015-07-14 16:52:19,559 - INFO - epoch 1 finished\n",
      "2015-07-14 16:52:31,760 - INFO - Iteration 3500: 1.22145929337\n",
      ".\n",
      "\n",
      "Thank you!  I was not conditional community and the reading least are the other comment in the record, and there is an old and what the most convex\n",
      "2015-07-14 16:53:09,654 - INFO - Iteration 3600: 1.20749376416\n",
      ". I could try the google sequence is a problem. Maybe you can't each on the theory of the bigger data, everyone treated some methods. The options is t\n",
      "2015-07-14 16:53:45,762 - INFO - Iteration 3700: 1.21376259685\n",
      ". What do you mean by the map for a bunch of finite students the first time serialism the resdemand are either if you have about making a classifier t\n",
      "2015-07-14 16:54:20,866 - INFO - Iteration 3800: 1.21671120048\n",
      ".\n",
      "\n",
      "http://www.amazon.com/Kimmands/Amplar_Computer_All-Max_mathematics)*\n",
      "\n",
      "Even we're each with a decent source for the size. I don't know of anyone to \n",
      "2015-07-14 16:54:56,220 - INFO - Iteration 3900: 1.19710858941\n",
      ". I would only have one study for me in its certain term with the word students for a supervised learning default, (PM is shotten valuable in the cont\n",
      "2015-07-14 16:55:30,967 - INFO - Iteration 4000: 1.2005098021\n",
      ". Thanks an interesting test set, they are someone has to say your description is the deep learning algorithms. The fact here are good thoughts about \n",
      "2015-07-14 16:55:31,359 - INFO - Saving net to: /tmp/char_4000.h5\n",
      "2015-07-14 16:55:31,380 - INFO - Saving figure to: train_loss.jpg\n",
      "2015-07-14 16:56:05,731 - INFO - Iteration 4100: 1.18019378066\n",
      ". I always another use itself in the frequency code is to read up priors for this discussion of the training set correctly is possible to have a look \n",
      "2015-07-14 16:56:40,474 - INFO - Iteration 4200: 1.19560232997\n",
      ".  \n",
      "\n",
      "I will do with the full lines. That is little... I could use the top of the areas in the confaudent of vision. \n",
      "\n",
      "that is that you can ask a limit\n",
      "2015-07-14 16:57:15,398 - INFO - Iteration 4300: 1.17861832619\n",
      ". If you have a lot of data in the convnets. There are a lot of the text first could be interesting? That seems that it's a bit being perfectly but wh\n",
      "2015-07-14 16:57:50,818 - INFO - Iteration 4400: 1.19398228884\n",
      ". It's not saying that there is transform to run out there and then use it to the convolutional networks are an online like same topic with more than \n",
      "2015-07-14 16:58:26,186 - INFO - Iteration 4500: 1.18785483718\n",
      ".  I'm always good at the field where the human brain does and values in statistics are based on the redistributed as a domain good way to get studyin\n",
      "2015-07-14 16:59:01,738 - INFO - Iteration 4600: 1.1865261662\n",
      ". The expectation of the closest algorithm present about simple toolbox. So package, large deep learning are discipuing for great interview:\n",
      "\n",
      "\"The pro\n",
      "2015-07-14 16:59:37,607 - INFO - Iteration 4700: 1.183168782\n",
      ". This is a great point of the other math interest you try to show the amount of time subs right now, but I'd like to get a simplified feedback in the\n",
      "2015-07-14 17:00:14,981 - INFO - Iteration 4800: 1.1796555686\n",
      ".  I think the \"hearing parameters\" and normally distribution, what a highlight is pretty simple as I was looking at their problem, that's a somewhat \n",
      "2015-07-14 17:01:11,986 - INFO - Iteration 4900: 1.18285028338\n",
      ". I would like to know what is this algorithm \"the best of the article.\n",
      "\n",
      "So if you have a subset of the focultime to do it.\n",
      "\n",
      "I'm not sure what you wan\n",
      "2015-07-14 17:02:15,041 - INFO - Iteration 5000: 1.18186717749\n",
      ".  This is the way to detect a sense in the last year library compared to the beginning of the best set of things well to be honest in the user/camero\n",
      "2015-07-14 17:02:16,005 - INFO - Saving net to: /tmp/char_5000.h5\n",
      "2015-07-14 17:02:16,025 - INFO - Saving figure to: train_loss.jpg\n",
      "2015-07-14 17:03:17,926 - INFO - Iteration 5100: 1.16133283377\n",
      ".?. I don't think it's supposed to have the answer in this step, seriously, I would guess that shape off the advised search - despine an experience is\n",
      "2015-07-14 17:04:20,132 - INFO - Iteration 5200: 1.15898365498\n",
      "... It will work with a long time to do a lot of linear categories (or linear) is the distribution of time to mention files in the best guy with lots \n",
      "2015-07-14 17:04:22,112 - INFO - epoch 2 finished\n",
      "2015-07-14 17:05:22,369 - INFO - Iteration 5300: 1.15320243001\n",
      ".  What do you mean by \"integrated to control do anything like this post that has different into your own internet set is with the same which has been\n",
      "2015-07-14 17:06:25,130 - INFO - Iteration 5400: 1.16002966046\n",
      ". Even how do you figured are types of not for a scholar...\n",
      "\n",
      "I really enjoyed your response size. I found to read the chance of the default for the sp\n",
      "2015-07-14 17:07:27,632 - INFO - Iteration 5500: 1.15576643586\n",
      ". I've not seen that a good training on expression of a start. That's too language (not a single net*. It looks like there is a lot of svfdilling and \n",
      "2015-07-14 17:08:30,182 - INFO - Iteration 5600: 1.14415872097\n",
      ". Is the connection to the first feature in pattern, the app of probabilities, but I guess it teaching as a few things to take a modified time series \n",
      "2015-07-14 17:09:27,060 - INFO - Iteration 5700: 1.14454081535\n",
      ".  the first question, it's difficult to implement a subreddit (self-cuder-comments/ending+correlation)\n",
      "\n",
      "http://en.wikipedia.org/wiki/Andrew_\n",
      "\n",
      "Not try\n",
      "2015-07-14 17:10:09,799 - INFO - Iteration 5800: 1.13972764373\n",
      ". The task and confidence that their use used better and control and mathematics, the weights are explored developers and use a different article, he \n",
      "2015-07-14 17:10:52,707 - INFO - Iteration 5900: 1.14059593678\n",
      "...\n",
      "\n",
      "I mean some areas of series of classifier? Any big data scientists are defined to prediction and decreases for a simpler dataset and then waiting\n",
      "2015-07-14 17:11:55,042 - INFO - Iteration 6000: 1.13109241128\n",
      ". Now many good entities can be a good place to use their data, then what's the author when you're reviewing from the explanations are more efficient \n",
      "2015-07-14 17:11:56,243 - INFO - Saving net to: /tmp/char_6000.h5\n",
      "2015-07-14 17:11:56,262 - INFO - Saving figure to: train_loss.jpg\n",
      "2015-07-14 17:12:57,717 - INFO - Iteration 6100: 1.1386946404\n",
      ". I'm guessing the basically they are basically working on it. They aren't really a problem. Then it sometimis itself and attract the proposal. There \n",
      "2015-07-14 17:13:59,919 - INFO - Iteration 6200: 1.14018576026\n",
      ". No, it is not a bit though. :)\n",
      "\n",
      "* [Curse Market) and Statistics are far the advance in not support for a statistics and caret projects, the universi\n",
      "2015-07-14 17:15:02,788 - INFO - Iteration 6300: 1.12355326056\n",
      ". Anyone else that I could add and wadn't perhaps much about the traditional neural network with the problem that allows to mention this on the algori\n",
      "2015-07-14 17:16:04,929 - INFO - Iteration 6400: 1.1280440855\n",
      ". I have read about Lecunifier to Reddit and probably the formatting service to the despine of the summer. As I mean sircach of the problem retired of\n",
      "2015-07-14 17:17:07,169 - INFO - Iteration 6500: 1.13643162012\n",
      ". It has a fun to a machine with his project on this topic for the code for the features of colorisories with a notable formatting. What the problem i\n",
      "2015-07-14 17:18:10,475 - INFO - Iteration 6600: 1.12754042149\n",
      ".\n",
      "\n",
      "I'm not sure what a good scholars not using facelualy westerday? I haven't feel a lot of the same thing not to be a probability because I have some\n",
      "2015-07-14 17:18:58,578 - INFO - Iteration 6700: 1.12661787748\n",
      ".\n",
      "\n",
      "This is awesome. \n",
      "\n",
      "This is a good thing.  The full description was definitely meaning to ask the machine learning submission of science in the meas\n",
      "2015-07-14 17:20:00,499 - INFO - Iteration 6800: 1.13398642898\n",
      ". It takes a lot of SAE where the thread with the computing in Figure learning python for the data mining and problems when the layer sentences is a b\n",
      "2015-07-14 17:21:01,490 - INFO - Iteration 6900: 1.12320473552\n",
      "... /r/mlbooks. I don't know what i think is to a definite world.  I found a cool training data resolezin so is that the chance in R is that the trith\n",
      "2015-07-14 17:21:23,992 - INFO - epoch 3 finished\n",
      "2015-07-14 17:22:05,033 - INFO - Iteration 7000: 1.13231443584\n",
      ". And what you are looking for. The statistic graphics of the company in ML have a spiniole features (fold codes like the test set\" in the post.  It's\n",
      "2015-07-14 17:22:06,273 - INFO - Saving net to: /tmp/char_7000.h5\n",
      "2015-07-14 17:22:06,293 - INFO - Saving figure to: train_loss.jpg\n",
      "2015-07-14 17:23:08,027 - INFO - Iteration 7100: 1.13518686533\n",
      ". I just want to do through the project that been asked machine learning and even less your GPU and do some actually the examples are the first time I\n",
      "2015-07-14 17:24:11,943 - INFO - Iteration 7200: 1.12479353368\n",
      "... a while ago. I really do see the same sentence of the string post to think. I might check through it before I believe that the short time is inter\n",
      "2015-07-14 17:25:14,987 - INFO - Iteration 7300: 1.12928579688\n",
      ". What are you referring to and is an explorere in the CS and there is an introduction to the languages and started with ML, A Modified Machine course\n",
      "2015-07-14 17:26:17,282 - INFO - Iteration 7400: 1.11995182514\n",
      "... warning on this article. The way the analogy is a significant site of the paper that work for the article. The optimization is a similar situation\n",
      "2015-07-14 17:27:19,557 - INFO - Iteration 7500: 1.11698162794\n",
      ". I would love to make a professor when the supervised learning but in ML as my argument for being too and there are some time to actual financial wor\n",
      "2015-07-14 17:28:24,922 - INFO - Iteration 7600: 1.10459698915\n",
      ". I've done the standard final one. Any technological machine learning algorithms is trivial? How did it be done a little lately interesting. The lear\n",
      "2015-07-14 17:29:29,384 - INFO - Iteration 7700: 1.10143353343\n",
      "... only the company that has been attempting to want something like this. Anyone who has heard of it with the forum time and the CVPR for the fact th\n",
      "2015-07-14 17:30:33,655 - INFO - Iteration 7800: 1.09902300775\n",
      ". The training set is overly deeps for some weird techniques. The work is something in the paper in Machine Learning For deep-learning data and applyi\n",
      "2015-07-14 17:31:35,618 - INFO - Iteration 7900: 1.10368838668\n",
      ". I was a fan in the context of the problem, but it sounds like you could have a link in the data mining that comes by here because the model is not r\n",
      "2015-07-14 17:32:38,162 - INFO - Iteration 8000: 1.09127886176\n",
      "...I have worked on a terribatic parameter for a Maximum normalization problem so that I was happy to know if you have a company to make it worked tha\n",
      "2015-07-14 17:32:39,464 - INFO - Saving net to: /tmp/char_8000.h5\n",
      "2015-07-14 17:32:39,484 - INFO - Saving figure to: train_loss.jpg\n",
      "2015-07-14 17:33:40,170 - INFO - Iteration 8100: 1.09283560634\n",
      ". The description is the world, but I want to be in the main intuition to an anyone for the data and this is fairly different.  *  It would always be \n",
      "2015-07-14 17:34:42,103 - INFO - Iteration 8200: 1.09824183583\n",
      ". I'm not suring well that the difference is the improvements and learning, but I don't know how you could start from the internal operations in hours\n",
      "2015-07-14 17:35:45,289 - INFO - Iteration 8300: 1.09271856487\n",
      "... I have some instances, and it really needs to be a project as well, it should be surgest data and a planet in Python.  \n",
      "\n",
      "They don't get a little o\n",
      "2015-07-14 17:36:49,028 - INFO - Iteration 8400: 1.0942524457\n",
      ". It's are alwairing the same thing between the ML course for Linux - it depends on the problem is what they are using the latest videos. I spent the \n",
      "2015-07-14 17:37:51,913 - INFO - Iteration 8500: 1.09399008512\n",
      ". I like this paper and still even make the original the confusion model is only a bit buzzword for me at a lot of the course on the primary professor\n",
      "2015-07-14 17:38:54,157 - INFO - Iteration 8600: 1.08715297222\n",
      ".  \n",
      "\n",
      "At a company that can be tried in this context? Even if you can do things along with the term \"data\" mathematics\" graduate data from the list of \n",
      "2015-07-14 17:39:38,006 - INFO - epoch 4 finished\n",
      "2015-07-14 17:39:56,928 - INFO - Iteration 8700: 1.09278958201\n",
      ". I was thinking about the last year's lab.  As far as I can tell, I'm missing the discussion of the deep learning problem. There is also future the n\n",
      "2015-07-14 17:40:57,291 - INFO - Iteration 8800: 1.08665896297\n",
      ".  In each equival manual language is designed to perform the answer you'll need to as well a lot of people tode the input of the top of math and data\n",
      "2015-07-14 17:41:59,773 - INFO - Iteration 8900: 1.09426155031\n",
      ".\n",
      "\n",
      "Let's say it has a lot of them.  Thank you. I haven't heard anything in the videos for disk and a pan of algorithms than convnets.  What are your t\n",
      "2015-07-14 17:43:04,373 - INFO - Iteration 9000: 1.09643367231\n",
      ". The reason for community has an emergence is a very good idea. I would think this is a pretty discussing models on a method, completely thought it w\n",
      "2015-07-14 17:43:05,410 - INFO - Saving net to: /tmp/char_9000.h5\n",
      "2015-07-14 17:43:05,430 - INFO - Saving figure to: train_loss.jpg\n",
      "2015-07-14 17:44:09,573 - INFO - Iteration 9100: 1.08294484735\n",
      ".  The community here.\n",
      "\n",
      "You might fall from the architecture to start as the experiments and the paragraph and a novice that it's a fairly basic one, \n",
      "2015-07-14 17:45:14,274 - INFO - Iteration 9200: 1.08127765119\n",
      "... what i don't seem to be a bit their next by a collection of way to deploy one request results or not. You can do is trying to train a random fores\n",
      "2015-07-14 17:46:18,346 - INFO - Iteration 9300: 1.07532514751\n",
      ". I looked at the same thing? If anyone can compute science and it's really not inefficient than regression on the profile of the classifier and publi\n",
      "2015-07-14 17:47:20,330 - INFO - Iteration 9400: 1.08894033909\n",
      ". For instructed bybran described a level of feature or optimisation. I start in both of the details. There's the real famous as your embedding for de\n",
      "2015-07-14 17:48:22,961 - INFO - Iteration 9500: 1.07417552471\n",
      ". What kind of small detection is a nast test set that offer based detection in each opinion and it included finsting alihing the state of my computer\n",
      "2015-07-14 17:49:25,420 - INFO - Iteration 9600: 1.0853093189\n",
      ". I've gotten it well probably in the structure nodes and self-public article. Just to get the more for classification then there is one sort of thing\n",
      "2015-07-14 17:50:27,653 - INFO - Iteration 9700: 1.07992377818\n",
      "... I totally agree with the idea. The most important thing is that they are sticking to determine the neural networks. There are partially a few more\n",
      "2015-07-14 17:51:28,748 - INFO - Iteration 9800: 1.07938737035\n",
      ". Example, how do you do it using this GPU. Most people who are still on the organization, but it's really helpful. I'm very interested in this system\n",
      "2015-07-14 17:52:32,943 - INFO - Iteration 9900: 1.07599914789\n",
      ". So is that deep learning and search for visualizations of contracts are not clear when I was looking for, but use neural networks is pretty informat\n",
      "2015-07-14 17:53:35,758 - INFO - Iteration 10000: 1.07671613753\n",
      ".  You might want to look at the videos, but I didn't see the code and they are bugt for the same time, I would assume that the probability does not p\n",
      "2015-07-14 17:53:37,031 - INFO - Saving net to: /tmp/char_10000.h5\n",
      "2015-07-14 17:53:37,051 - INFO - Saving figure to: train_loss.jpg\n"
     ]
    }
   ],
   "source": [
    "for i in range(hyper['max_iter']):\n",
    "    train_loss_hist.append(forward(net, sentence_batches))\n",
    "    net.backward()\n",
    "    lr = (hyper['base_lr'] * (hyper['gamma'])**(i // hyper['stepsize']))\n",
    "    net.update(lr=lr, momentum=hyper['momentum'],\n",
    "        clip_gradients=hyper['clip_gradients'], weight_decay=hyper['weight_decay'])\n",
    "    if i % hyper['display_interval'] == 0:\n",
    "        logging.info('Iteration %d: %s' % (i, np.mean(train_loss_hist[-hyper['display_interval']:])))\n",
    "    if i % hyper['test_interval'] == 0:\n",
    "        eval_performance(net)\n",
    "    if i % hyper['snapshot_interval'] == 0 and i > 0:\n",
    "        filename = '%s_%d.h5' % (hyper['snapshot_prefix'], i)\n",
    "        logging.info('Saving net to: %s' % filename)\n",
    "        net.save(filename)\n",
    "    if i % hyper['graph_interval'] == 0 and i > 0:\n",
    "        sub = 100\n",
    "        plt.plot(np.convolve(train_loss_hist, np.ones(sub)/sub)[sub:-sub])\n",
    "        filename = '%strain_loss.jpg' % hyper['graph_prefix']\n",
    "        logging.info('Saving figure to: %s' % filename)\n",
    "        plt.savefig(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
